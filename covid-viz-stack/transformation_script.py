#!/usr/bin/env python3
"""
Script de transformation pour la surveillance √©pid√©miologique
Nettoie, enrichit et optimise les donn√©es brutes
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import logging
import pyarrow as pa
import pyarrow.parquet as pq

# Configuration
DATA_DIR = Path("data")
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"

# Pays d'int√©r√™t (codes ISO)
TARGET_COUNTRIES = [
    'FRA',  # France
    'DEU',  # Allemagne  
    'ITA',  # Italie
    'ESP',  # Espagne
    'GBR',  # Royaume-Uni
    'BEL',  # Belgique
    'NLD',  # Pays-Bas
    'OWID_WRL'  # Monde (agr√©g√©)
]

# Colonnes d'int√©r√™t
CORE_COLUMNS = [
    'iso_code', 'location', 'date', 'population',
    'total_cases', 'new_cases', 'total_deaths', 'new_deaths',
    'total_vaccinations', 'people_vaccinated', 'people_fully_vaccinated',
    'new_vaccinations', 'stringency_index'
]

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_raw_data():
    """Charge les donn√©es brutes les plus r√©centes"""
    try:
        # Cherche le fichier le plus r√©cent ou utilise le lien symbolique
        latest_file = RAW_DIR / "latest_owid_covid_data.csv"
        
        if not latest_file.exists():
            # Fallback : cherche le fichier le plus r√©cent
            raw_files = list(RAW_DIR.glob("owid_covid_data_*.csv"))
            if not raw_files:
                raise FileNotFoundError("Aucun fichier de donn√©es brutes trouv√©")
            latest_file = max(raw_files, key=lambda x: x.stat().st_mtime)
        
        logger.info(f"Chargement des donn√©es depuis : {latest_file}")
        df = pd.read_csv(latest_file)
        
        logger.info(f"Donn√©es charg√©es : {len(df):,} lignes, {len(df.columns)} colonnes")
        return df
        
    except Exception as e:
        logger.error(f"Erreur lors du chargement : {e}")
        raise

def filter_and_clean(df):
    """Filtre les pays d'int√©r√™t et nettoie les donn√©es de base"""
    try:
        logger.info("D√©but du filtrage et nettoyage")
        
        # 1. Filtrer les pays d'int√©r√™t
        df_filtered = df[df['iso_code'].isin(TARGET_COUNTRIES)].copy()
        logger.info(f"Apr√®s filtrage pays : {len(df_filtered):,} lignes")
        
        # 2. Garder seulement les colonnes utiles
        available_columns = [col for col in CORE_COLUMNS if col in df_filtered.columns]
        df_filtered = df_filtered[available_columns]
        logger.info(f"Colonnes conserv√©es : {len(available_columns)}")
        
        # 3. Conversion et nettoyage des dates
        df_filtered['date'] = pd.to_datetime(df_filtered['date'])
        
        # 4. Filtrage temporel (2 derni√®res ann√©es)
        cutoff_date = datetime.now() - timedelta(days=730)
        df_filtered = df_filtered[df_filtered['date'] >= cutoff_date]
        logger.info(f"Apr√®s filtrage temporel (depuis {cutoff_date.date()}) : {len(df_filtered):,} lignes")
        
        # 5. Tri par pays et date
        df_filtered = df_filtered.sort_values(['iso_code', 'date']).reset_index(drop=True)
        
        return df_filtered
        
    except Exception as e:
        logger.error(f"Erreur lors du filtrage : {e}")
        raise

def handle_missing_values(df):
    """Gestion intelligente des valeurs manquantes"""
    try:
        logger.info("Gestion des valeurs manquantes")
        
        # Log initial des valeurs manquantes
        missing_info = df.isnull().sum()
        logger.info("Valeurs manquantes par colonne :")
        for col, count in missing_info.items():
            if count > 0:
                pct = (count / len(df)) * 100
                logger.info(f"  {col}: {count:,} ({pct:.1f}%)")
        
        # Strat√©gies par type de colonne
        
        # 1. Colonnes cumulatives : forward fill par pays
        cumulative_cols = ['total_cases', 'total_deaths', 'total_vaccinations', 
                          'people_vaccinated', 'people_fully_vaccinated']
        
        for col in cumulative_cols:
            if col in df.columns:
                df[col] = df.groupby('iso_code')[col].fillna(method='ffill')
        
        # 2. Colonnes quotidiennes : 0 pour les valeurs manquantes
        daily_cols = ['new_cases', 'new_deaths', 'new_vaccinations']
        
        for col in daily_cols:
            if col in df.columns:
                df[col] = df[col].fillna(0)
        
        # 3. Population : forward fill (ne change pas souvent)
        if 'population' in df.columns:
            df['population'] = df.groupby('iso_code')['population'].fillna(method='ffill')
        
        # 4. Stringency index : interpolation
        if 'stringency_index' in df.columns:
            df['stringency_index'] = df.groupby('iso_code')['stringency_index'].fillna(method='ffill')
        
        logger.info("‚úÖ Gestion des valeurs manquantes termin√©e")
        return df
        
    except Exception as e:
        logger.error(f"Erreur lors de la gestion des valeurs manquantes : {e}")
        raise

def calculate_derived_metrics(df):
    """Calcule des m√©triques d√©riv√©es utiles pour l'analyse"""
    try:
        logger.info("Calcul des m√©triques d√©riv√©es")
        
        # 1. Taux d'incidence pour 100k habitants
        if 'new_cases' in df.columns and 'population' in df.columns:
            df['incidence_rate_100k'] = (df['new_cases'] / df['population']) * 100000
        
        # 2. Taux de mortalit√© pour 100k habitants
        if 'new_deaths' in df.columns and 'population' in df.columns:
            df['death_rate_100k'] = (df['new_deaths'] / df['population']) * 100000
        
        # 3. Case Fatality Rate (CFR)
        if 'total_deaths' in df.columns and 'total_cases' in df.columns:
            df['case_fatality_rate'] = np.where(
                df['total_cases'] > 0,
                (df['total_deaths'] / df['total_cases']) * 100,
                0
            )
        
        # 4. Moyennes mobiles sur 7 jours
        rolling_cols = ['new_cases', 'new_deaths', 'incidence_rate_100k']
        
        for col in rolling_cols:
            if col in df.columns:
                new_col_name = f'{col}_7day_avg'
                df[new_col_name] = df.groupby('iso_code')[col].rolling(
                    window=7, center=True, min_periods=1
                ).mean().reset_index(drop=True)
        
        # 5. Pourcentage de population vaccin√©e
        if 'people_fully_vaccinated' in df.columns and 'population' in df.columns:
            df['vaccination_rate'] = np.where(
                df['population'] > 0,
                (df['people_fully_vaccinated'] / df['population']) * 100,
                0
            )
        
        logger.info(f"‚úÖ M√©triques d√©riv√©es calcul√©es. Nouvelles colonnes : {len(df.columns)}")
        return df
        
    except Exception as e:
        logger.error(f"Erreur lors du calcul des m√©triques : {e}")
        raise

def generate_quality_report(df):
    """G√©n√®re un rapport de qualit√© des donn√©es"""
    try:
        logger.info("G√©n√©ration du rapport de qualit√©")
        
        report = {
            'timestamp': datetime.now(),
            'total_rows': len(df),
            'countries': df['iso_code'].nunique(),
            'date_range': {
                'start': df['date'].min().date(),
                'end': df['date'].max().date(),
                'days': (df['date'].max() - df['date'].min()).days
            },
            'data_completeness': {}
        }
        
        # Compl√©tude par colonne
        for col in df.columns:
            if col not in ['iso_code', 'location', 'date']:
                total_values = len(df)
                non_null_values = df[col].notna().sum()
                completeness = (non_null_values / total_values) * 100
                report['data_completeness'][col] = round(completeness, 1)
        
        # Sauvegarder le rapport
        report_file = PROCESSED_DIR / f"quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        import json
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        # Log du r√©sum√©
        logger.info(f"Rapport de qualit√© :")
        logger.info(f"  - Lignes: {report['total_rows']:,}")
        logger.info(f"  - Pays: {report['countries']}")
        logger.info(f"  - P√©riode: {report['date_range']['start']} √† {report['date_range']['end']}")
        
        return report
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration du rapport : {e}")
        raise

def save_processed_data(df):
    """Sauvegarde les donn√©es trait√©es en format Parquet"""
    try:
        logger.info("Sauvegarde des donn√©es trait√©es")
        
        # Nom du fichier avec timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        parquet_file = PROCESSED_DIR / f"covid_processed_{timestamp}.parquet"
        
        # Sauvegarde en Parquet avec compression
        df.to_parquet(
            parquet_file,
            compression='snappy',
            index=False
        )
        
        # Cr√©er un lien symbolique vers le fichier le plus r√©cent
        latest_link = PROCESSED_DIR / "latest_covid_processed.parquet"
        if latest_link.exists():
            latest_link.unlink()
        latest_link.symlink_to(parquet_file.name)
        
        # Statistiques du fichier
        file_size_mb = parquet_file.stat().st_size / (1024 * 1024)
        logger.info(f"‚úÖ Donn√©es sauvegard√©es : {parquet_file}")
        logger.info(f"Taille du fichier : {file_size_mb:.2f} MB")
        logger.info(f"Compression ratio vs CSV : ~70-80%")
        
        return parquet_file
        
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde : {e}")
        raise

def main():
    """Fonction principale de transformation"""
    try:
        logger.info("üîÑ D√©but du processus de transformation")
        
        # 1. Charger les donn√©es brutes
        df = load_raw_data()
        
        # 2. Filtrer et nettoyer
        df = filter_and_clean(df)
        
        # 3. G√©rer les valeurs manquantes
        df = handle_missing_values(df)
        
        # 4. Calculer les m√©triques d√©riv√©es
        df = calculate_derived_metrics(df)
        
        # 5. G√©n√©rer le rapport de qualit√©
        quality_report = generate_quality_report(df)
        
        # 6. Sauvegarder les donn√©es trait√©es
        output_file = save_processed_data(df)
        
        logger.info("‚úÖ Processus de transformation termin√© avec succ√®s")
        logger.info(f"Fichier de sortie : {output_file}")
        
        return df, quality_report
        
    except Exception as e:
        logger.error(f"‚ùå √âchec du processus de transformation : {e}")
        raise

if __name__ == "__main__":
    main()