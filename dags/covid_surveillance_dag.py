from datetime import datetime, timedelta
from pathlib import Path

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.email import EmailOperator
from airflow.sensors.filesystem import FileSensor
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup

import pandas as pd
import requests
import logging
import sys

# Configuration du DAG
DAG_ID = 'covid_surveillance_pipeline'
SCHEDULE_INTERVAL = '0 6 * * *'  # Tous les jours √† 6h du matin
START_DATE = days_ago(1)

# Configuration des chemins
BASE_DIR = Path('/opt/airflow/data')  # Chemin dans le container Airflow
RAW_DIR = BASE_DIR / 'raw'
PROCESSED_DIR = BASE_DIR / 'processed'

# Configuration des donn√©es
RAW_DATA_URL = "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv"

# Arguments par d√©faut pour les t√¢ches
default_args = {
    'owner': 'data-engineering-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'email': ['admin@company.com'],  # √Ä remplacer par votre email
}

def setup_directories():
    """Cr√©e la structure des dossiers n√©cessaire"""
    for directory in [RAW_DIR, PROCESSED_DIR]:
        directory.mkdir(parents=True, exist_ok=True)
    logging.info("Structure des dossiers cr√©√©e/v√©rifi√©e")

def check_data_source_availability():
    """V√©rifie que la source de donn√©es est accessible"""
    try:
        response = requests.head(RAW_DATA_URL, timeout=10)
        if response.status_code == 200:
            logging.info("‚úÖ Source de donn√©es accessible")
            return True
        else:
            raise Exception(f"Source inaccessible, status code: {response.status_code}")
    except Exception as e:
        logging.error(f"‚ùå Source de donn√©es inaccessible: {e}")
        raise

def download_covid_data():
    """T√©l√©charge les donn√©es COVID depuis Our World in Data"""
    try:
        logging.info(f"D√©but du t√©l√©chargement depuis {RAW_DATA_URL}")
        
        # T√©l√©chargement
        response = requests.get(RAW_DATA_URL, timeout=300)  # 5 min timeout
        response.raise_for_status()
        
        # G√©n√©ration du nom de fichier avec timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"owid_covid_data_{timestamp}.csv"
        filepath = RAW_DIR / filename
        
        # Sauvegarde
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        # Cr√©ation du lien symbolique
        latest_link = RAW_DIR / "latest_owid_covid_data.csv"
        if latest_link.exists():
            latest_link.unlink()
        latest_link.symlink_to(filename)
        
        file_size_mb = filepath.stat().st_size / (1024*1024)
        logging.info(f"‚úÖ Donn√©es t√©l√©charg√©es: {filepath} ({file_size_mb:.2f} MB)")
        
        return str(filepath)
        
    except Exception as e:
        logging.error(f"‚ùå Erreur lors du t√©l√©chargement: {e}")
        raise

def validate_raw_data():
    """Valide les donn√©es t√©l√©charg√©es"""
    try:
        latest_file = RAW_DIR / "latest_owid_covid_data.csv"
        
        if not latest_file.exists():
            raise FileNotFoundError("Fichier de donn√©es non trouv√©")
        
        # Lecture et validation
        df = pd.read_csv(latest_file)
        
        # V√©rifications critiques
        required_columns = ['date', 'location', 'iso_code', 'total_cases', 'new_cases']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            raise ValueError(f"Colonnes manquantes: {missing_columns}")
        
        # V√©rification de la fra√Æcheur des donn√©es
        df['date'] = pd.to_datetime(df['date'])
        latest_date = df['date'].max()
        days_old = (datetime.now().date() - latest_date.date()).days
        
        if days_old > 7:
            logging.warning(f"‚ö†Ô∏è Donn√©es anciennes: {days_old} jours")
        
        # Statistiques de validation
        stats = {
            'rows': len(df),
            'countries': df['location'].nunique(),
            'date_range': f"{df['date'].min().date()} √† {df['date'].max().date()}",
            'data_age_days': days_old
        }
        
        logging.info(f"‚úÖ Validation r√©ussie: {stats}")
        return stats
        
    except Exception as e:
        logging.error(f"‚ùå √âchec de la validation: {e}")
        raise

def transform_data():
    """Transforme les donn√©es brutes"""
    try:
        sys.path.append("/opt/airflow/")
        from scripts.transformation_script import main as transform_main
        
        # Ex√©cute le script de transformation
        df, quality_report = transform_main()
        
        # Log des r√©sultats
        logging.info(f"‚úÖ Transformation termin√©e:")
        logging.info(f"  - Lignes trait√©es: {len(df):,}")
        logging.info(f"  - Pays: {df['iso_code'].nunique()}")
        logging.info(f"  - Colonnes: {len(df.columns)}")
        
        return quality_report
        
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la transformation: {e}")
        raise

def data_quality_checks():
    """Effectue des v√©rifications de qualit√© sur les donn√©es transform√©es"""
    try:
        latest_file = PROCESSED_DIR / "latest_covid_processed.parquet"
        
        if not latest_file.exists():
            raise FileNotFoundError("Fichier de donn√©es transform√©es non trouv√©")
        
        df = pd.read_parquet(latest_file)
        
        # Tests de qualit√©
        quality_issues = []
        
        # 1. V√©rification des valeurs n√©gatives dans les nouveaux cas
        negative_cases = df[df['new_cases'] < 0]
        if len(negative_cases) > 0:
            quality_issues.append(f"Valeurs n√©gatives dans new_cases: {len(negative_cases)} lignes")
        
        # 2. V√©rification de la coh√©rence des donn√©es cumulatives
        for country in df['iso_code'].unique():
            country_data = df[df['iso_code'] == country].sort_values('date')
            if 'total_cases' in country_data.columns:
                # V√©rifier que les totaux sont croissants (ou stables)
                decreasing = country_data['total_cases'].diff() < -1000  # Tol√©rance pour corrections
                if decreasing.any():
                    quality_issues.append(f"Totaux d√©croissants d√©tect√©s pour {country}")
        
        # 3. V√©rification des taux calcul√©s
        if 'incidence_rate_100k' in df.columns:
            extreme_rates = df[df['incidence_rate_100k'] > 1000]  # Plus de 1000 cas / 100k habitants
            if len(extreme_rates) > 0:
                quality_issues.append(f"Taux d'incidence extr√™mes: {len(extreme_rates)} observations")
        
        # R√©sultats
        if quality_issues:
            logging.warning(f"‚ö†Ô∏è Probl√®mes de qualit√© d√©tect√©s:")
            for issue in quality_issues:
                logging.warning(f"  - {issue}")
        else:
            logging.info("‚úÖ Tous les tests de qualit√© sont pass√©s")
        
        quality_summary = {
            'timestamp': datetime.now().isoformat(),
            'total_rows': len(df),
            'quality_issues': quality_issues,
            'status': 'WARNING' if quality_issues else 'OK'
        }
        
        return quality_summary
        
    except Exception as e:
        logging.error(f"‚ùå Erreur lors des v√©rifications qualit√©: {e}")
        raise

def send_success_notification(**context):
    """Envoie une notification de succ√®s"""
    execution_date = context['ds']
    task_instance = context['task_instance']
    
    # R√©cup√©ration des statistiques des t√¢ches pr√©c√©dentes
    validation_stats = task_instance.xcom_pull(task_ids='ingestion_group.validate_raw_data')
    quality_report = task_instance.xcom_pull(task_ids='data_quality_checks')
    
    message = f"""
    ‚úÖ Pipeline de surveillance √©pid√©miologique ex√©cut√© avec succ√®s
    
    üìÖ Date d'ex√©cution: {execution_date}
    üìä Donn√©es trait√©es: {validation_stats.get('rows', 'N/A')} lignes
    üåç Pays couverts: {validation_stats.get('countries', 'N/A')}
    üìà P√©riode: {validation_stats.get('date_range', 'N/A')}
    
    üîç Statut qualit√©: {quality_report.get('status', 'N/A')}
    
    Les donn√©es sont disponibles dans: {PROCESSED_DIR}/latest_covid_processed.parquet
    """
    
    logging.info("üìß Notification de succ√®s envoy√©e")
    return message

# Cr√©ation du DAG
dag = DAG(
    DAG_ID,
    default_args=default_args,
    description='Pipeline de surveillance √©pid√©miologique COVID-19',
    schedule_interval=SCHEDULE_INTERVAL,
    start_date=START_DATE,
    catchup=False,  # Ne pas ex√©cuter les runs manqu√©s
    max_active_runs=1,  # Une seule ex√©cution √† la fois
    tags=['epidemiologie', 'covid', 'data-engineering'],
)

# === D√âFINITION DES T√ÇCHES ===

# T√¢che de pr√©paration
setup_task = PythonOperator(
    task_id='setup_directories',
    python_callable=setup_directories,
    dag=dag,
)

# Groupe de t√¢ches d'ingestion
with TaskGroup('ingestion_group', dag=dag) as ingestion_group:
    
    check_source_task = PythonOperator(
        task_id='check_data_source',
        python_callable=check_data_source_availability,
        dag = dag
    )
    
    download_task = PythonOperator(
        task_id='download_data',
        python_callable=download_covid_data,
        dag = dag
    )
    
    validate_task = PythonOperator(
        task_id='validate_raw_data',
        python_callable=validate_raw_data,
        dag = dag
    )
    
    check_source_task >> download_task >> validate_task

# T√¢che de transformation
transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)

# T√¢che de v√©rification qualit√©
quality_check_task = PythonOperator(
    task_id='data_quality_checks',
    python_callable=data_quality_checks,
    dag=dag,
)

# T√¢che de notification
notification_task = PythonOperator(
    task_id='send_success_notification',
    python_callable=send_success_notification,
    dag=dag,
)

# T√¢che de nettoyage (optionnelle)
cleanup_task = BashOperator(
    task_id='cleanup_old_files',
    bash_command=f"""
    # Supprime les fichiers raw de plus de 7 jours
    find {RAW_DIR} -name "owid_covid_data_*.csv" -mtime +7 -delete || true
    # Supprime les fichiers processed de plus de 30 jours  
    find {PROCESSED_DIR} -name "covid_processed_*.parquet" -mtime +30 -delete || true
    echo "Nettoyage termin√©"
    """,
    dag=dag,
)

# === D√âFINITION DES D√âPENDANCES ===

setup_task >> ingestion_group >> transform_task >> quality_check_task >> notification_task >> cleanup_task